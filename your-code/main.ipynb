{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsnCPbdkxYZd"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <h1 style=\"color: #FF6347;\">Self-Guided Lab: Retrieval-Augmented Generation (RAGs)</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.13/site-packages (1.0.8)\n",
            "Requirement already satisfied: langchain_community in /opt/anaconda3/lib/python3.13/site-packages (0.4.1)\n",
            "Requirement already satisfied: pypdf in /opt/anaconda3/lib/python3.13/site-packages (6.3.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.13/site-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (4.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (2.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.13/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.7.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (2.0.39)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=2.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_community) (2.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.6->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: termcolor in /opt/anaconda3/lib/python3.13/site-packages (3.2.0)\n",
            "Requirement already satisfied: langchain_openai in /opt/anaconda3/lib/python3.13/site-packages (1.0.3)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: chromadb in /opt/anaconda3/lib/python3.13/site-packages (1.3.5)\n",
            "Requirement already satisfied: langchain_chroma in /opt/anaconda3/lib/python3.13/site-packages (1.0.0)\n",
            "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.13/site-packages (0.12.0)\n",
            "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.13/site-packages (2.8.1)\n",
            "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.13/site-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.2 in /opt/anaconda3/lib/python3.13/site-packages (from langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.13/site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.13/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.13/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.4.45)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-core<2.0.0,>=1.0.2->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.2->langchain_openai) (2.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.2->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n",
            "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /opt/anaconda3/lib/python3.13/site-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
            "  Using cached sentence_transformers-4.0.0-py3-none-any.whl.metadata (13 kB)\n",
            "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-2.0.0.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.2.1.tar.gz (80 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.2.0.tar.gz (81 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.1.1.tar.gz (81 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.1.0.tar.gz (78 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.4.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.3.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.2.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.1.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-1.0.0.tar.gz (74 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.2.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.1.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.1.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.4.0.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.9.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.8.tar.gz (66 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.3.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.7.2.tar.gz (59 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.7.1.tar.gz (59 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.7.tar.gz (59 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.6.tar.gz (62 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers<3.2.0,>=3.1.0 (from sentence-transformers)\n",
            "  Using cached transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.5.1.tar.gz (61 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==3.0.2 (from sentence-transformers)\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.3.5.tar.gz (61 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.4.tar.gz (61 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.3.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.2.tar.gz (65 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.1.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.3.0.tar.gz (61 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.6.2.tar.gz (60 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.11.0 (from sentence-transformers)\n",
            "  Using cached transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.6.1.tar.gz (55 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.5.1.tar.gz (52 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.3.0 (from sentence-transformers)\n",
            "  Using cached transformers-2.3.0-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.5.tar.gz (49 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.4.1.tar.gz (49 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers==2.2.1 (from sentence-transformers)\n",
            "  Using cached transformers-2.2.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.4.tar.gz (49 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.3.tar.gz (45 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pytorch-transformers==1.1.0 (from sentence-transformers)\n",
            "  Using cached pytorch_transformers-1.1.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.2.tar.gz (44 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.2.1.tar.gz (42 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pytorch-transformers==1.0.0 (from sentence-transformers)\n",
            "  Using cached pytorch_transformers-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentence-transformers\n",
            "  Using cached sentence-transformers-0.2.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Using cached sentence-transformers-0.1.0.tar.gz (35 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tokenizers<1.0.0,>=0.19.1 (from langchain-huggingface)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface)\n",
            "  Using cached hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n",
            "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Using cached langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions<5,>=4.11 (from openai)\n",
            "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
            "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting pydantic<3,>=1.9.0 (from openai)\n",
            "  Using cached pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
            "Collecting packaging<26.0.0,>=23.2.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached langsmith-0.4.45-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.2->langchain_openai)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.0.2 (from langchain_openai)\n",
            "  Using cached langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jiter<1,>=0.10.0 (from openai)\n",
            "  Using cached jiter-0.12.0-cp313-cp313-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai)\n",
            "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting openai\n",
            "  Using cached openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.12.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting langchain_openai\n",
            "  Using cached langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting termcolor\n",
            "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "\u001b[31mERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0, sentence-transformers==2.1.0, sentence-transformers==2.2.0, sentence-transformers==2.2.1, sentence-transformers==2.2.2, sentence-transformers==2.3.0, sentence-transformers==2.3.1, sentence-transformers==2.4.0, sentence-transformers==2.5.0, sentence-transformers==2.5.1, sentence-transformers==2.6.0, sentence-transformers==2.6.1, sentence-transformers==2.7.0, sentence-transformers==3.0.0, sentence-transformers==3.0.1, sentence-transformers==3.1.0, sentence-transformers==3.1.1, sentence-transformers==3.2.0, sentence-transformers==3.2.1, sentence-transformers==3.3.0, sentence-transformers==3.3.1, sentence-transformers==3.4.0, sentence-transformers==3.4.1, sentence-transformers==4.0.0, sentence-transformers==4.0.1, sentence-transformers==4.0.2, sentence-transformers==4.1.0, sentence-transformers==5.0.0, sentence-transformers==5.1.0, sentence-transformers==5.1.1 and sentence-transformers==5.1.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    sentence-transformers 5.1.2 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.1.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 5.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.2 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 4.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.4.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.4.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.3.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.3.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.2.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.2.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.1.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.1.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.0.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 3.0.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.7.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.6.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.6.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.5.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.5.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.4.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.3.1 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.3.0 depends on torch>=1.11.0\n",
            "    sentence-transformers 2.2.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.2.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.2.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.1.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 2.0.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.2.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.2.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.1.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.1.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.4 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.3 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 1.0.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1.2 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.1 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.4.0 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.3.9 depends on torch>=1.6.0\n",
            "    sentence-transformers 0.3.8 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7.2 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.7 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.6 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.5.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.5 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.4 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.3 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.2 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.1 depends on torch>=1.2.0\n",
            "    sentence-transformers 0.3.0 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.6.2 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.6.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.5.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.5 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.4.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.4 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.3 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.2 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.1 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.2.0 depends on torch>=1.0.1\n",
            "    sentence-transformers 0.1.0 depends on torch>=1.0.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "%pip install langchain langchain_community pypdf\n",
        "%pip install termcolor langchain_openai langchain-huggingface sentence-transformers chromadb langchain_chroma tiktoken openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZp4BQAVxYZj"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ3FsdzRveTBrenMxM3VnbDMwaTJxN2NnZm50aGFibXk1NzNnY2Q0MCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LR5ZBwZHv02lmpVoEU/giphy.gif\" alt=\"NLP Gif\" style=\"width: 300px; height: 150px; object-fit: cover; object-position: center;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizk6HCYxYZo"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Data Storage & Retrieval</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5UOI8ZxYZp"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">PyPDFLoader</h2>\n",
        "\n",
        "`PyPDFLoader` is a lightweight Python library designed to streamline the process of loading and parsing PDF documents for text processing tasks. It is particularly useful in Retrieval-Augmented Generation workflows where text extraction from PDFs is required.\n",
        "\n",
        "- **What Does PyPDFLoader Do?**\n",
        "  - Extracts text from PDF files, retaining formatting and layout.\n",
        "  - Simplifies the preprocessing of document-based datasets.\n",
        "  - Supports efficient and scalable loading of large PDF collections.\n",
        "\n",
        "- **Key Features:**\n",
        "  - Compatible with popular NLP libraries and frameworks.\n",
        "  - Handles multi-page PDFs and embedded images (e.g., OCR-compatible setups).\n",
        "  - Provides flexible configurations for structured text extraction.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Preparing PDF documents for retrieval-based systems in RAGs.\n",
        "  - Automating the text extraction pipeline for document analysis.\n",
        "  - Creating datasets from academic papers, technical manuals, and reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install(package):\n",
        "    print(f\"Installing {package}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# List of (import_name, package_name)\n",
        "packages = [\n",
        "    (\"openai\", \"openai\"),\n",
        "    (\"dotenv\", \"python-dotenv\"),\n",
        "    (\"langchain_community\", \"langchain-community\"),\n",
        "    (\"pypdf\", \"pypdf\"),\n",
        "    (\"langchain_text_splitters\", \"langchain-text-splitters\"),\n",
        "    (\"langchain_openai\", \"langchain-openai\"),\n",
        "    (\"langchain_chroma\", \"langchain-chroma\"),\n",
        "    (\"termcolor\", \"termcolor\"),\n",
        "    (\"tiktoken\", \"tiktoken\")\n",
        "]\n",
        "\n",
        "for import_name, package_name in packages:\n",
        "    try:\n",
        "        __import__(import_name)\n",
        "    except ImportError:\n",
        "        install(package_name)\n",
        "\n",
        "# Now perform the actual imports for usage\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from termcolor import colored\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "print(\"All libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cuREtJRixYZt"
      },
      "outputs": [],
      "source": [
        "# File path for the document\n",
        "file_path = \"../ai-for-everyone.pdf\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz_8SOLxxYZt"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Documents into pages</h3>\n",
        "\n",
        "The `PyPDFLoader` library allows efficient loading and splitting of PDF documents into smaller, manageable parts for NLP tasks.\n",
        "\n",
        "This functionality is particularly useful in workflows requiring granular text processing, such as Retrieval-Augmented Generation (RAG).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_b5Z_45UxYZu",
        "outputId": "a600d69f-14fe-4492-f236-97261d6ff36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "297"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and split the document\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load_and_split()\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt50NRQaxYZv"
      },
      "source": [
        "<h3 style=\"color: #FF8C00;\">Pages into Chunks</h3>\n",
        "\n",
        "\n",
        "####  RecursiveCharacterTextSplitter in LangChain\n",
        "\n",
        "The `RecursiveCharacterTextSplitter` is the **recommended splitter** in LangChain when you want to break down long documents into smaller, semantically meaningful chunks â€” especially useful in **RAG pipelines**, where clean context chunks lead to better LLM responses.\n",
        "\n",
        "####  Parameters\n",
        "\n",
        "| Parameter       | Description                                                                 |\n",
        "|-----------------|-----------------------------------------------------------------------------|\n",
        "| `chunk_size`    | The **maximum number of characters** allowed in a chunk (e.g., `1000`).     |\n",
        "| `chunk_overlap` | The number of **overlapping characters** between consecutive chunks (e.g., `200`). This helps preserve context continuity. |\n",
        "\n",
        "####  How it works\n",
        "`RecursiveCharacterTextSplitter` attempts to split the text **intelligently**, trying the following separators in order:\n",
        "1. Paragraphs (`\"\\n\\n\"`)\n",
        "2. Lines (`\"\\n\"`)\n",
        "3. Sentences or words (`\" \"`)\n",
        "4. Individual characters (as a last resort)\n",
        "\n",
        "This makes it ideal for handling **natural language documents**, such as PDFs, articles, or long reports, without breaking sentences or paragraphs in awkward ways.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1096"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Alternative: CharacterTextSplitter\n",
        "\n",
        "`CharacterTextSplitter` is a simpler splitter that breaks text into chunks based **purely on character count**, without trying to preserve any natural language structure.\n",
        "\n",
        "##### Example:\n",
        "```python\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "````\n",
        "\n",
        "This method is faster and more predictable but may split text in the middle of a sentence or paragraph, which can hurt performance in downstream tasks like retrieval or QA.\n",
        "\n",
        "---\n",
        "\n",
        "#### Comparison Table\n",
        "\n",
        "| Feature                        | RecursiveCharacterTextSplitter | CharacterTextSplitter     |\n",
        "| ------------------------------ | ------------------------------ | ------------------------- |\n",
        "| Structure-aware splitting      |  Yes                          |  No                      |\n",
        "| Preserves sentence/paragraphs  |  Yes                          |  No                      |\n",
        "| Risk of splitting mid-sentence |  Minimal                     |  High                   |\n",
        "| Ideal for RAG/document QA      |  Highly recommended           |  Only if structured text |\n",
        "| Performance speed              |  Slightly slower             |  Faster                  |\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendation\n",
        "\n",
        "Use `RecursiveCharacterTextSplitter` for most real-world document processing tasks, especially when building RAG pipelines or working with structured natural language content like PDFs or articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices for Choosing Chunk Size in RAG\n",
        "\n",
        "### Best Practices for Chunk Size in RAG\n",
        "\n",
        "| Factor                      | Recommendation                                                                                                                                                                                          |\n",
        "| ---------------------------| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **LLM context limit**       | Choose a chunk size that lets you retrieve multiple chunks **without exceeding the modelâ€™s token limit**. For example, GPT-4o supports 128k tokens, but with GPT-3.5 (16k) or GPT-4 (32k), keep it modest. |\n",
        "| **Chunk size (in characters)** | Typically: **500â€“1,000 characters** per chunk â†’ ~75â€“200 tokens. This fits well for retrieval + prompt without context overflow.                                                                           |\n",
        "| **Chunk size (in tokens)**  | If using token-based splitter (e.g. `TokenTextSplitter`): aim for **100â€“300 tokens** per chunk.                                                                                                            |\n",
        "| **Chunk overlap**           | Use **overlap of 10â€“30%** (e.g., 100â€“300 characters or ~50 tokens) to preserve context across chunk boundaries and avoid cutting off important ideas mid-sentence.                                        |\n",
        "| **Document structure**      | Use **`RecursiveCharacterTextSplitter`** to preserve semantic boundaries (paragraphs, sentences) instead of arbitrary cuts.                                                                                |\n",
        "| **Task type**               | For **question answering**, smaller chunks (~500â€“800 chars) reduce noise.<br>For **summarization**, slightly larger chunks (~1000â€“1500) are OK.                                                          |\n",
        "| **Embedding model**         | Some models (e.g., `text-embedding-3-large`) can handle long input. But still, smaller chunks give **finer-grained retrieval**, which improves relevance.                                                  |\n",
        "| **Query type**              | If users ask **very specific questions**, small focused chunks are better. For broader queries, bigger chunks might help.                                                                                  |\n",
        "\n",
        "\n",
        "### Rule of Thumb\n",
        "\n",
        "| Use Case                 | Chunk Size      | Overlap |\n",
        "| ------------------------| --------------- | ------- |\n",
        "| Factual Q&A              | 500â€“800 chars   | 100â€“200 |\n",
        "| Summarization            | 1000â€“1500 chars | 200â€“300 |\n",
        "| Technical documents      | 400â€“700 chars   | 100â€“200 |\n",
        "| Long reports/books       | 800â€“1200 chars  | 200â€“300 |\n",
        "| Small LLMs (â‰¤16k tokens) | â‰¤800 chars      | 100â€“200 |\n",
        "\n",
        "\n",
        "### Avoid\n",
        "\n",
        "- Chunks >2000 characters: risks context overflow.\n",
        "- No overlap: may lose key information between chunks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg15RjVPxYZw"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Embeddings</h2>\n",
        "\n",
        "Embeddings transform text into dense vector representations, capturing semantic meaning and contextual relationships. They are essential for efficient document retrieval and similarity analysis.\n",
        "\n",
        "- **What are OpenAI Embeddings?**\n",
        "  - Pre-trained embeddings like `text-embedding-3-large` generate high-quality vector representations for text.\n",
        "  - Encapsulate semantic relationships in the text, enabling robust NLP applications.\n",
        "\n",
        "- **Key Features of `text-embedding-3-large`:**\n",
        "  - Large-scale embedding model optimized for accuracy and versatility.\n",
        "  - Handles diverse NLP tasks, including retrieval, classification, and clustering.\n",
        "  - Ideal for applications with high-performance requirements.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Reduces the need for extensive custom training.\n",
        "  - Provides state-of-the-art performance in retrieval-augmented systems.\n",
        "  - Compatible with RAGs to create powerful context-aware models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MNZfTng5xYZz",
        "outputId": "db1a7c85-ef9f-447e-92cd-9d097e959847"
      },
      "outputs": [],
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", openai_api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsSA7RKvxYZz"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChromaDB</h2>\n",
        "\n",
        "ChromaDB is a versatile vector database designed for efficiently storing and retrieving embeddings. It integrates seamlessly with embedding models to enable high-performance similarity search and context-based retrieval.\n",
        "\n",
        "### Workflow Overview:\n",
        "- **Step 1:** Generate embeddings using a pre-trained model (e.g., OpenAI's `text-embedding-3-large`).\n",
        "- **Step 2:** Store the embeddings in ChromaDB for efficient retrieval and similarity calculations.\n",
        "- **Step 3:** Use the stored embeddings to perform searches, matching, or context-based retrieval.\n",
        "\n",
        "### Key Features of ChromaDB:\n",
        "- **Scalability:** Handles large-scale datasets with optimized indexing and search capabilities.\n",
        "- **Speed:** Provides fast and accurate retrieval of embeddings for real-time applications.\n",
        "- **Integration:** Supports integration with popular frameworks and libraries for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VkjHR-RkxYZ0",
        "outputId": "bc11bda9-f283-457a-f584-5a06b95c4dd9"
      },
      "outputs": [
        {
          "ename": "InternalError",
          "evalue": "Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(chunks, embeddings, persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_db_LAB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChromaDB created with document embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1377\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mif\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m   1378\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m   1379\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m   1380\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m   1381\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m   1382\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m   1383\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m   1384\u001b[0m     host\u001b[38;5;241m=\u001b[39mhost,\n\u001b[1;32m   1385\u001b[0m     port\u001b[38;5;241m=\u001b[39mport,\n\u001b[1;32m   1386\u001b[0m     ssl\u001b[38;5;241m=\u001b[39mssl,\n\u001b[1;32m   1387\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1388\u001b[0m     chroma_cloud_api_key\u001b[38;5;241m=\u001b[39mchroma_cloud_api_key,\n\u001b[1;32m   1389\u001b[0m     tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m   1390\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m   1391\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m   1392\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m   1393\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m   1394\u001b[0m     collection_configuration\u001b[38;5;241m=\u001b[39mcollection_configuration,\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1396\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:1311\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, host, port, headers, chroma_cloud_api_key, tenant, database, client_settings, client, collection_metadata, collection_configuration, ssl, **kwargs)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m   1306\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m   1307\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m   1308\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m   1310\u001b[0m     ):\n\u001b[0;32m-> 1311\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m   1312\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m   1313\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1315\u001b[0m         )\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:644\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    645\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39membeddings_with_metadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts_with_metadatas,\n\u001b[1;32m    648\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids_with_metadata,\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/chromadb/api/models/Collection.py:458\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m upsert_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_upsert_request(\n\u001b[1;32m    450\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    451\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n\u001b[1;32m    456\u001b[0m )\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_upsert(\n\u001b[1;32m    459\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    460\u001b[0m     ids\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    461\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    462\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    463\u001b[0m     documents\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    464\u001b[0m     uris\u001b[38;5;241m=\u001b[39mupsert_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    465\u001b[0m     tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    466\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    467\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/chromadb/api/rust.py:498\u001b[0m, in \u001b[0;36mRustBindingsAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_upsert\u001b[39m(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    497\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbindings\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[1;32m    500\u001b[0m         ids,\n\u001b[1;32m    501\u001b[0m         embeddings,\n\u001b[1;32m    502\u001b[0m         metadatas,\n\u001b[1;32m    503\u001b[0m         documents,\n\u001b[1;32m    504\u001b[0m         uris,\n\u001b[1;32m    505\u001b[0m         tenant,\n\u001b[1;32m    506\u001b[0m         database,\n\u001b[1;32m    507\u001b[0m     )\n",
            "\u001b[0;31mInternalError\u001b[0m: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db_LAB\")\n",
        "print(\"ChromaDB created with document embeddings.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27OdN1IVxYZ1"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Retrieving Documents</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice1: Write a user question that someone might ask about your bookâ€™s topic or content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiLv-TfrxYZ1"
      },
      "outputs": [],
      "source": [
        "user_question = \"What is the impact of AI on society?\" # User question\n",
        "retrieved_docs = db.similarity_search(user_question, k=3) # k is the number of documents to retrieve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgWsh50JxYZ1",
        "outputId": "c8640c5d-5955-471f-fdd2-37096f5f68c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            " jobs and the changing nature of work has \n",
            "been one prominent area where AI is said to have dramatic implications. Work-\n",
            "ers are subjected to evermore data collection about not just their activities at \n",
            "work, but beyond factors related to work. At the same time, machine learning \n",
            "systems are using this data to transform how work is being allocated, assessed \n",
            "and completed. Often it is these two components â€“ data collection and machine \n",
            "learning â€“ that is referred to under the banner of AI (SÃ¡nchez-Monedero and \n",
            "Dencik 2019). This has a profound impact on workersâ€™ lives, the nature of jobs \n",
            "and the economy. Moreover, the position of labour in relation to AI brings to \n",
            "light the social stratifications embedded within and created by the advance -\n",
            "ment of AI across social life. AI extends long-standing debates on modes of \n",
            "capitalism that significantly shape the circumstances of working people whilst\n",
            "Document 2:\n",
            " jobs and the changing nature of work has \n",
            "been one prominent area where AI is said to have dramatic implications. Work-\n",
            "ers are subjected to evermore data collection about not just their activities at \n",
            "work, but beyond factors related to work. At the same time, machine learning \n",
            "systems are using this data to transform how work is being allocated, assessed \n",
            "and completed. Often it is these two components â€“ data collection and machine \n",
            "learning â€“ that is referred to under the banner of AI (SÃ¡nchez-Monedero and \n",
            "Dencik 2019). This has a profound impact on workersâ€™ lives, the nature of jobs \n",
            "and the economy. Moreover, the position of labour in relation to AI brings to \n",
            "light the social stratifications embedded within and created by the advance -\n",
            "ment of AI across social life. AI extends long-standing debates on modes of \n",
            "capitalism that significantly shape the circumstances of working people whilst\n",
            "Document 3:\n",
            "1st century. (ibid. 2)\n",
            "The High-Level Expert Group on Artificial Intelligence (AI HLEG) goes into \n",
            "even greater detail about the capabilities of AI to make humanity â€˜flourishâ€™ , thus \n",
            "solving all problems of society.\n",
            "We believe that AI has the potential to significantly transform society. AI \n",
            "is not an end in itself, but rather a promising means to increase human \n",
            "flourishing, thereby enhancing individual and societal well-being and \n",
            "the common good, as well as bringing progress and innovation. In par-\n",
            "ticular, AI systems can help to facilitate the achievement of the UNâ€™s \n",
            "Sustainable Development Goals, such as promoting gender balance \n",
            "and tackling climate change, rationalising our use of natural resources, \n",
            "enhancing our health, mobility and production processes, and support-\n",
            "ing how we monitor progress against sustainability and social cohesion \n",
            "indicators. (High-Level Expert Group 2019a, 4)\n"
          ]
        }
      ],
      "source": [
        "# Display top results\n",
        "for i, doc in enumerate(retrieved_docs[:3]): # Display top 3 results\n",
        "    print(f\"Document {i+1}:\\n{doc.page_content[36:1000]}\") # Display content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuGK8gL6xYZ1"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">Preparing Content for GenAI</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iB3lZqHxYZ2"
      },
      "outputs": [],
      "source": [
        "def _get_document_prompt(docs):\n",
        "    prompt = \"\\n\"\n",
        "    for doc in docs:\n",
        "        prompt += \"\\nContent:\\n\"\n",
        "        prompt += doc.page_content + \"\\n\\n\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2okzmuADxYZ2",
        "outputId": "0aa6cdca-188d-40e0-f5b4-8888d3549ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context formatted for GPT model.\n"
          ]
        }
      ],
      "source": [
        "# Generate a formatted context from the retrieved documents\n",
        "formatted_context = _get_document_prompt(retrieved_docs)\n",
        "print(\"Context formatted for GPT model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIczQNTxYZ2"
      },
      "source": [
        "<h2 style=\"color: #FF8C00;\">ChatBot Architecture</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice2: Write a prompt that is relevant and tailored to the content and style of your book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqxVh9s3xYZ3",
        "outputId": "97cca95d-4ab3-44d8-a76c-5713aad387d8"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "You are an expert on Artificial Intelligence. Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{formatted_context}\n",
        "\n",
        "Question:\n",
        "{user_question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice3: Tune parameters like temperature, and penalties to control how creative, focused, or varied the model's responses are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylypRWRlxYZ4"
      },
      "outputs": [],
      "source": [
        "# Set up GPT client and parameters\n",
        "client = openai.OpenAI()\n",
        "model_params = {\n",
        "    'model': 'gpt-4o',\n",
        "    'temperature': 0.7,  # Increase creativity\n",
        "    'max_tokens': 500,  # Allow for longer responses\n",
        "    'top_p': 1.0,        # Use nucleus sampling\n",
        "    'frequency_penalty': 0.0,  # Reduce repetition\n",
        "    'presence_penalty': 0.0   # Encourage new topics\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8e942xDxYZ4"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Response</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eXZO4pIxYZ4"
      },
      "outputs": [],
      "source": [
        "messages = [{'role': 'user', 'content': prompt}]\n",
        "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLPAcchBxYZ5",
        "outputId": "976c7800-16ed-41fe-c4cf-58f60d3230d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The impact of AI on society is multifaceted and significant. AI has the potential to transform various aspects of human life and work by altering how jobs are allocated, assessed, and completed. This transformation is driven by increased data collection and machine learning, which are central components of AI. As AI systems become more integrated into workplaces, they can affect workers' lives, the nature of jobs, and the economy at large. \n",
            "\n",
            "Moreover, AI influences social stratifications by highlighting and sometimes exacerbating existing inequalities within societies. The advancement of AI also extends debates around modes of capitalism and the changing nature of labor, potentially reshaping the circumstances of working people.\n",
            "\n",
            "On a more positive note, AI is seen as a means to enhance human flourishing and societal well-being. It can contribute to achieving the United Nations' Sustainable Development Goals by promoting gender balance, tackling climate change, optimizing resource use, improving health, mobility, and production processes, and supporting the monitoring of sustainability and social cohesion indicators. Thus, AI holds the promise of progress and innovation that can benefit humanity.\n"
          ]
        }
      ],
      "source": [
        "answer = completion.choices[0].message.content\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXVNXPwLxYaT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:824/1*GK56xmDIWtNQAD_jnBIt2g.png\" alt=\"NLP Gif\" style=\"width: 500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldybhlqKxYaT"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Cosine Similarity</h2>\n",
        "\n",
        "**Cosine similarity** is a metric used to measure the alignment or similarity between two vectors, calculated as the cosine of the angle between them. It is the **most common metric used in RAG pipelines** for vector retrieval.. It provides a scale from -1 to 1:\n",
        "\n",
        "- **-1**: Vectors are completely opposite.\n",
        "- **0**: Vectors are orthogonal (uncorrelated or unrelated).\n",
        "- **1**: Vectors are identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1I1TNhxYaT"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg\" alt=\"NLP Gif\" style=\"width: 700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoEMdNgQxYaU"
      },
      "source": [
        "<h2 style=\"color: #FF6347;\">Keyword Highlighting</h2>\n",
        "\n",
        "Highlighting important keywords helps users quickly understand the relevance of the retrieved text to their query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwDyofY0xYaV"
      },
      "source": [
        "The `highlight_keywords` function is designed to highlight specific keywords within a given text. It replaces each keyword in the text with a highlighted version using the `colored` function from the `termcolor` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y3E0YWExYaV"
      },
      "outputs": [],
      "source": [
        "def highlight_keywords(text, keywords):\n",
        "    for keyword in keywords:\n",
        "        text = text.replace(keyword, colored(keyword, 'green', attrs=['bold']))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice4: add your keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7SkWPpnxYaW",
        "outputId": "28e82563-edba-4b41-acad-ec27e5ba134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet 1:\n",
            "societal challenges. The question of jobs and the changing nature of work has \n",
            "been one prominent area where \u001b[1m\u001b[32mAI\u001b[0m is said to have dramatic implications. Work-\n",
            "ers are subjected to evermore data collecti\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "query_keywords = ['AI', 'society', 'impact', 'artificial intelligence'] # add your keywords\n",
        "for i, doc in enumerate(retrieved_docs[:1]):\n",
        "    snippet = doc.page_content[:200]\n",
        "    highlighted = highlight_keywords(snippet, query_keywords)\n",
        "    print(f\"Snippet {i+1}:\\n{highlighted}\\n{'-'*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhV_Jf_LxYaX"
      },
      "source": [
        "1. `query_keywords` is a list of keywords to be highlighted.\n",
        "2. The loop iterates over the first document in retrieved_docs.\n",
        "3. For each document, a snippet of the first 200 characters is extracted.\n",
        "4. The highlight_keywords function is called to highlight the keywords in the snippet.\n",
        "5. The highlighted snippet is printed along with a separator line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBRKysAvxYaX"
      },
      "source": [
        "<h1 style=\"color: #FF6347;\">Bonus</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj25lCybxYaX"
      },
      "source": [
        "**Try loading one of your own PDF books and go through the steps again to explore how the pipeline works with your content**:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
